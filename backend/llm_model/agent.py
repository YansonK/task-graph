import os
from .tools import tools, TaskBreakdownSignature
import dspy
import json
import logging
from typing import AsyncGenerator, Dict, Any
from openai import AsyncOpenAI
import asyncio

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

max_iters = 5

class Agent:
    def __init__(self):
        # Get OpenAI API key from environment
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        # Check if key is a placeholder
        if 'your_openai_api_key_here' in api_key.lower() or 'sk-' not in api_key:
            raise ValueError(f"OPENAI_API_KEY appears to be invalid. Please set a real OpenAI API key in backend/.env")

        logger.info(f"Initializing Agent with OpenAI API key: {api_key[:10]}...")

        # Store API key for direct OpenAI client usage
        self.api_key = api_key

        # Initialize AsyncOpenAI client for streaming
        self.openai_client = AsyncOpenAI(api_key=api_key)

        # Configure DSPy with OpenAI GPT-4o-mini
        dspy.configure(lm=dspy.LM('openai/gpt-4o-mini', api_key=api_key))
        self.react_agent = dspy.ReAct(
            TaskBreakdownSignature,
            tools=list(tools.values()),
            max_iters=max_iters
        )

    def query(self, chat_history, graph_data):
        """
        Non-streaming query method for backward compatibility.

        Args:
            chat_history: List of chat messages
            graph_data: Current graph state with nodes and links

        Returns:
            Agent result with response and updated graph
        """
        result = self.react_agent(
            conversation_history=chat_history,
            task_nodes=graph_data
        )

        # Process tool calls and update graph
        for i in range(max_iters):
            current_tool = f"tool_name_{i}"
            tool_result = f'observation_{i}'

            if current_tool not in result.trajectory:
                break

            match result.trajectory[current_tool]:
                case "create_task_node":
                    # Create a new task node
                    node_data = result.trajectory[tool_result]

                    # Check if this is an error message
                    if isinstance(node_data, str) and ("error" in node_data.lower() or "execution error" in node_data.lower()):
                        logger.error(f"Tool execution failed: {node_data}")
                        break

                    # Parse if it's a string (JSON or dict representation)
                    if isinstance(node_data, str):
                        try:
                            node = json.loads(node_data)
                        except json.JSONDecodeError:
                            # Try eval as fallback (for dict string representation)
                            try:
                                import ast
                                node = ast.literal_eval(node_data)
                            except (SyntaxError, ValueError) as e:
                                logger.error(f"Failed to parse node data: {node_data}. Error: {e}")
                                break
                    else:
                        node = node_data

                    # Validate node has required fields
                    if not isinstance(node, dict) or "id" not in node or "name" not in node:
                        logger.error(f"Invalid node data: {node}")
                        break

                    if len(graph_data["nodes"]) > 0 and node.get("parent_id"):
                        graph_data["links"].append({
                            "source": node["parent_id"],
                            "target": node["id"]
                        })

                    graph_data["nodes"].append({
                        "id": node["id"],
                        "name": node["name"],
                        "description": node["description"]
                    })
                case _:
                    break

        return result

    async def query_stream(self, chat_history, graph_data) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Streaming query method using DSPy's ReAct agent with TRUE real-time streaming.

        This provides intelligent multi-step reasoning with tokens streamed as generated.

        Yields:
            Dict with 'type' and corresponding data:
            - {'type': 'token', 'content': str} - Text chunk as generated by the model
            - {'type': 'graph_update', 'graph_data': dict} - Updated graph
        """
        try:
            # Queue to communicate between sync ReAct and async streaming
            import queue
            import threading

            stream_queue = queue.Queue()
            agent_done = threading.Event()
            agent_result = {}
            agent_error = {}

            # Custom streaming callback
            class StreamingLM(dspy.LM):
                def __init__(self, model, api_key, stream_queue):
                    super().__init__(model=model, api_key=api_key)
                    self.api_key = api_key  # Store api_key as instance attribute
                    self.stream_queue = stream_queue
                    self.base_client = AsyncOpenAI(api_key=api_key)
                    self.call_count = 0  # Track which LM call we're on

                def parse_thinking_content(self, text):
                    """Parse thinking text and strip headers, return formatted content"""
                    import re

                    # Remove header markers and format content
                    sections = []

                    # Extract next_thought
                    thought_match = re.search(r'\[\[ ## next_thought ## \]\](.*?)(?=\[\[|$)', text, re.DOTALL)
                    if thought_match:
                        thought = thought_match.group(1).strip()
                        if thought:
                            sections.append(f"ðŸ’­ {thought}")

                    # Extract next_tool_name
                    tool_match = re.search(r'\[\[ ## next_tool_name ## \]\](.*?)(?=\[\[|$)', text, re.DOTALL)
                    if tool_match:
                        tool = tool_match.group(1).strip().strip("'\"")
                        if tool and tool != 'finish':
                            sections.append(f"ðŸ”§ Tool: {tool}")

                    # Extract next_tool_args
                    args_match = re.search(r'\[\[ ## next_tool_args ## \]\](.*?)(?=\[\[|$)', text, re.DOTALL)
                    if args_match:
                        args = args_match.group(1).strip()
                        if args and args != '{}':
                            try:
                                args_obj = json.loads(args)
                                sections.append(f"ðŸ“‹ Args: {json.dumps(args_obj, indent=2)}")
                            except:
                                sections.append(f"ðŸ“‹ Args: {args}")

                    return '\n\n'.join(sections) if sections else None

                def __call__(self, prompt=None, messages=None, **kwargs):
                    """Override to capture and stream responses token-by-token"""
                    # Use the parent's __call__ but intercept for streaming
                    import openai
                    import re

                    self.call_count += 1

                    # Prepare the request
                    if messages is None and prompt:
                        messages = [{"role": "user", "content": prompt}]

                    # Detect if this is a thinking call or final response
                    # ReAct makes multiple calls - early ones are thinking, last is response
                    prompt_text = str(prompt or messages)
                    is_thinking = 'next_thought' in prompt_text or 'Thought' in prompt_text or self.call_count == 1

                    # Make streaming request to OpenAI directly
                    client = openai.OpenAI(api_key=self.api_key)
                    stream = client.chat.completions.create(
                        model=self.model.replace('openai/', ''),
                        messages=messages,
                        stream=True,
                        **kwargs
                    )

                    # State machine for real-time token filtering
                    full_response = ""
                    buffer = ""  # Buffer for detecting markers
                    current_field = None  # Track which field we're in
                    in_json_response = False  # Track if we're in a JSON response field

                    # Markers to detect (longest is 30 chars)
                    MAX_MARKER_LENGTH = 35
                    THINKING_MARKERS = ['[[ ## next_thought ## ]]', '[[ ## next_tool_name ## ]]', '[[ ## next_tool_args ## ]]']
                    RESPONSE_START = '[[ ## response ## ]]'
                    RESPONSE_END = '[[ ## completed ## ]]'
                    REASONING_MARKER = '[[ ## reasoning ## ]]'

                    def could_be_partial_marker(text):
                        """Check if text could be the start of a marker"""
                        if not text or not text.startswith('['):
                            return False
                        # Check if it could be the start of any marker
                        all_markers = THINKING_MARKERS + [RESPONSE_START, RESPONSE_END, REASONING_MARKER, '"response"']
                        return any(marker.startswith(text) for marker in all_markers)

                    # Stream tokens as they arrive
                    for chunk in stream:
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if delta.content:
                                token = delta.content
                                full_response += token
                                buffer += token

                                # Process buffer in a loop to handle multiple markers
                                while buffer:
                                    marker_found = False
                                    content_to_send = None

                                    # Check for thinking field markers
                                    for marker in THINKING_MARKERS:
                                        if marker in buffer:
                                            # Send content before marker
                                            parts = buffer.split(marker, 1)
                                            if parts[0] and current_field:
                                                content_to_send = parts[0]

                                            # Update field and add header
                                            if 'next_thought' in marker:
                                                current_field = 'thinking'
                                                if is_thinking:
                                                    self.stream_queue.put(('thinking', 'ðŸ’­ '))
                                            elif 'next_tool_name' in marker:
                                                current_field = 'tool_name'
                                                if is_thinking:
                                                    self.stream_queue.put(('thinking', '\n\nðŸ”§ Tool: '))
                                            elif 'next_tool_args' in marker:
                                                current_field = 'tool_args'
                                                if is_thinking:
                                                    self.stream_queue.put(('thinking', '\n\nðŸ“‹ Args: '))

                                            buffer = parts[1]
                                            marker_found = True
                                            break

                                    # Check for response markers
                                    if not marker_found and RESPONSE_START in buffer:
                                        parts = buffer.split(RESPONSE_START, 1)
                                        if parts[0] and current_field:
                                            content_to_send = parts[0]
                                        current_field = 'response'
                                        buffer = parts[1]
                                        marker_found = True

                                    if not marker_found and RESPONSE_END in buffer:
                                        parts = buffer.split(RESPONSE_END, 1)
                                        if parts[0] and current_field == 'response':
                                            content_to_send = parts[0]
                                        current_field = None
                                        buffer = parts[1]
                                        marker_found = True

                                    # Check for reasoning marker (skip it)
                                    if not marker_found and REASONING_MARKER in buffer:
                                        buffer = buffer.split(REASONING_MARKER, 1)[1]
                                        marker_found = True

                                    # Check for JSON response field
                                    if not marker_found and not is_thinking and '"response"' in buffer and not in_json_response:
                                        match = re.search(r'"response"\s*:\s*"', buffer)
                                        if match:
                                            in_json_response = True
                                            buffer = buffer[match.end():]
                                            current_field = 'response'
                                            marker_found = True

                                    # Send content before marker if any
                                    if content_to_send:
                                        content_to_send = content_to_send.strip()
                                        if content_to_send:
                                            if is_thinking and current_field in ['thinking', 'tool_name', 'tool_args']:
                                                self.stream_queue.put(('thinking', content_to_send))
                                            elif not is_thinking and current_field == 'response':
                                                self.stream_queue.put(('token', content_to_send))

                                    # If no marker found, decide what to send
                                    if not marker_found:
                                        # Keep small buffer to detect partial markers
                                        if len(buffer) > MAX_MARKER_LENGTH:
                                            # Check if end of buffer could be partial marker
                                            send_up_to = len(buffer) - MAX_MARKER_LENGTH
                                            # Find safe cutoff point
                                            for i in range(len(buffer) - 1, send_up_to - 1, -1):
                                                if could_be_partial_marker(buffer[i:]):
                                                    send_up_to = i
                                                    break

                                            to_send = buffer[:send_up_to]
                                            buffer = buffer[send_up_to:]

                                            if to_send and current_field:
                                                # Send to appropriate stream
                                                if is_thinking and current_field in ['thinking', 'tool_name', 'tool_args']:
                                                    self.stream_queue.put(('thinking', to_send))
                                                elif not is_thinking and current_field == 'response':
                                                    # Check for end quote in JSON responses
                                                    if in_json_response and '"' in to_send:
                                                        parts = to_send.split('"', 1)
                                                        if parts[0]:
                                                            self.stream_queue.put(('token', parts[0]))
                                                        in_json_response = False
                                                        buffer = '"' + parts[1] + buffer
                                                    else:
                                                        self.stream_queue.put(('token', to_send))

                                        # Exit loop if no more processing needed
                                        break

                    # Flush remaining buffer
                    if buffer and current_field:
                        # Clean up the buffer
                        cleaned = buffer.strip()

                        # Remove closing markers/quotes
                        if in_json_response and '"' in cleaned:
                            cleaned = cleaned.split('"')[0]

                        # Remove common suffixes
                        for suffix in [RESPONSE_END, '"}', "'}}",  '}']:
                            if cleaned.endswith(suffix):
                                cleaned = cleaned[:-len(suffix)].strip()

                        if cleaned:
                            if is_thinking and current_field in ['thinking', 'tool_name', 'tool_args']:
                                self.stream_queue.put(('thinking', cleaned))
                            elif current_field == 'response':
                                self.stream_queue.put(('token', cleaned))

                    # Log the complete response for debugging
                    logger.info(f"=== LM Call {self.call_count} Complete ===")
                    logger.info(f"Is Thinking: {is_thinking}, Field: {current_field}")
                    logger.info(f"Response Length: {len(full_response)} chars")
                    logger.info(f"Response: {full_response[:300]}...")
                    logger.info("=" * 50)

                    # Return in format DSPy expects
                    return [full_response]

            # Run ReAct agent in background thread with streaming LM
            def run_react_with_streaming():
                try:
                    # Create streaming LM instance
                    streaming_lm = StreamingLM(
                        model='openai/gpt-4o-mini',
                        api_key=self.api_key,
                        stream_queue=stream_queue
                    )

                    # Run ReAct with streaming LM
                    with dspy.context(lm=streaming_lm):
                        result = self.react_agent(
                            conversation_history=chat_history,
                            task_nodes=graph_data
                        )

                    agent_result['result'] = result
                except Exception as e:
                    agent_error['error'] = e
                    logger.error(f"ReAct agent error: {e}")
                finally:
                    agent_done.set()

            # Start the agent in background thread
            agent_thread = threading.Thread(target=run_react_with_streaming)
            agent_thread.start()

            # Stream tokens from queue as they arrive
            while not agent_done.is_set() or not stream_queue.empty():
                try:
                    # Try to get tokens from queue with shorter timeout for responsiveness
                    msg_type, content = stream_queue.get(timeout=0.02)
                    if msg_type == 'token':
                        yield {
                            'type': 'token',
                            'content': content
                        }
                    elif msg_type == 'thinking':
                        yield {
                            'type': 'thinking',
                            'content': content
                        }
                    elif msg_type == 'replace_response':
                        yield {
                            'type': 'replace_response',
                            'content': content
                        }
                except queue.Empty:
                    # No tokens available, minimal wait before checking again
                    await asyncio.sleep(0.001)

            # Wait for thread to complete
            agent_thread.join()

            # Check for errors
            if 'error' in agent_error:
                raise agent_error['error']

            # Process tool calls and update graph
            if 'result' in agent_result:
                result = agent_result['result']

                for i in range(max_iters):
                    current_tool = f"tool_name_{i}"
                    tool_result = f'observation_{i}'

                    if current_tool not in result.trajectory:
                        break

                    if result.trajectory[current_tool] == "create_task_node":
                        # Create a new task node
                        node_data = result.trajectory[tool_result]

                        # Check if this is an error message
                        if isinstance(node_data, str) and ("error" in node_data.lower() or "execution error" in node_data.lower()):
                            logger.error(f"Tool execution failed: {node_data}")
                            break

                        # Parse if it's a string (JSON or dict representation)
                        if isinstance(node_data, str):
                            try:
                                node = json.loads(node_data)
                            except json.JSONDecodeError:
                                # Try eval as fallback (for dict string representation)
                                try:
                                    import ast
                                    node = ast.literal_eval(node_data)
                                except (SyntaxError, ValueError) as e:
                                    logger.error(f"Failed to parse node data: {node_data}. Error: {e}")
                                    break
                        else:
                            node = node_data

                        # Validate node has required fields
                        if not isinstance(node, dict) or "id" not in node or "name" not in node:
                            logger.error(f"Invalid node data: {node}")
                            break

                        if len(graph_data["nodes"]) > 0 and node.get("parent_id"):
                            graph_data["links"].append({
                                "source": node["parent_id"],
                                "target": node["id"]
                            })

                        graph_data["nodes"].append({
                            "id": node["id"],
                            "name": node["name"],
                            "description": node["description"]
                        })

                        logger.info(f"Created task node: {node['name']}")

            # Send final graph update
            yield {
                'type': 'graph_update',
                'graph_data': graph_data
            }

        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield {
                'type': 'token',
                'content': f"Sorry, I encountered an error: {str(e)}"
            }
