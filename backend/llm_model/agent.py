import os
from .tools import tools, TaskBreakdownSignature
import dspy
import json
import logging
from typing import AsyncGenerator, Dict, Any
from openai import AsyncOpenAI

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

max_iters = 5

class Agent:
    def __init__(self):
        # Get OpenAI API key from environment
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        # Check if key is a placeholder
        if 'your_openai_api_key_here' in api_key.lower() or 'sk-' not in api_key:
            raise ValueError(f"OPENAI_API_KEY appears to be invalid. Please set a real OpenAI API key in backend/.env")

        logger.info(f"Initializing Agent with OpenAI API key: {api_key[:10]}...")

        # Store API key for direct OpenAI client usage
        self.api_key = api_key

        # Initialize AsyncOpenAI client for streaming
        self.openai_client = AsyncOpenAI(api_key=api_key)

        # Configure DSPy with OpenAI GPT-4o-mini
        dspy.configure(lm=dspy.LM('openai/gpt-4o-mini', api_key=api_key))
        self.react_agent = dspy.ReAct(
            TaskBreakdownSignature,
            tools=list(tools.values()),
            max_iters=max_iters
        )

    def query(self, chat_history, graph_data):
        """
        Non-streaming query method for backward compatibility.

        Args:
            chat_history: List of chat messages
            graph_data: Current graph state with nodes and links

        Returns:
            Agent result with response and updated graph
        """
        result = self.react_agent(
            conversation_history=chat_history,
            task_nodes=graph_data
        )

        # Process tool calls and update graph
        for i in range(max_iters):
            current_tool = f"tool_name_{i}"
            tool_result = f'observation_{i}'

            if current_tool not in result.trajectory:
                break

            match result.trajectory[current_tool]:
                case "create_task_node":
                    # Create a new task node
                    node_data = result.trajectory[tool_result]

                    # Check if this is an error message
                    if isinstance(node_data, str) and ("error" in node_data.lower() or "execution error" in node_data.lower()):
                        logger.error(f"Tool execution failed: {node_data}")
                        break

                    # Parse if it's a string (JSON or dict representation)
                    if isinstance(node_data, str):
                        try:
                            node = json.loads(node_data)
                        except json.JSONDecodeError:
                            # Try eval as fallback (for dict string representation)
                            try:
                                import ast
                                node = ast.literal_eval(node_data)
                            except (SyntaxError, ValueError) as e:
                                logger.error(f"Failed to parse node data: {node_data}. Error: {e}")
                                break
                    else:
                        node = node_data

                    # Validate node has required fields
                    if not isinstance(node, dict) or "id" not in node or "name" not in node:
                        logger.error(f"Invalid node data: {node}")
                        break

                    if len(graph_data["nodes"]) > 0 and node.get("parent_id"):
                        graph_data["links"].append({
                            "source": node["parent_id"],
                            "target": node["id"]
                        })

                    graph_data["nodes"].append({
                        "id": node["id"],
                        "name": node["name"],
                        "description": node["description"]
                    })
                case _:
                    break

        return result

    async def query_stream(self, chat_history, graph_data) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Streaming query method that yields response chunks and graph updates as they're generated.

        Yields:
            Dict with 'type' and corresponding data:
            - {'type': 'token', 'content': str} - Text chunk as generated by the model
            - {'type': 'graph_update', 'graph_data': dict} - Updated graph
            - {'type': 'done'} - Completion signal
        """
        # Define tools for OpenAI function calling
        tools_schema = [
            {
                "type": "function",
                "function": {
                    "name": "create_task_node",
                    "description": "Create a new task node in the task graph. Use this when breaking down tasks into subtasks.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task_name": {
                                "type": "string",
                                "description": "Name of the task"
                            },
                            "task_description": {
                                "type": "string",
                                "description": "Detailed description of the task"
                            },
                            "parent_id": {
                                "type": "string",
                                "description": "ID of the parent task (optional)"
                            }
                        },
                        "required": ["task_name", "task_description"]
                    }
                }
            }
        ]

        # Build the system message with task context
        system_message = {
            "role": "system",
            "content": f"""You are an AI assistant that helps break down tasks and manage a task graph.

Current task graph state:
{json.dumps(graph_data, indent=2)}

You can help users:
1. Break down complex tasks into smaller, manageable subtasks
2. Organize tasks in a hierarchical structure
3. Clarify requirements before creating new tasks

Guidelines:
- Guide users to be more specific before creating tasks
- Ask clarifying questions when tasks are vague
- Only create task nodes when you have clear, well-defined tasks
- Use the create_task_node function to add tasks to the graph
- Be conversational and helpful"""
        }

        # Convert chat history to OpenAI format and add system message
        messages = [system_message] + chat_history

        try:
            # Create streaming completion with tool calling support
            stream = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                tools=tools_schema,
                stream=True,
                temperature=0.7,
            )

            # Track tool calls as they're streamed
            tool_calls_map = {}  # Maps tool call index to accumulated data

            # Stream the response token by token
            async for chunk in stream:
                if not chunk.choices or len(chunk.choices) == 0:
                    continue

                delta = chunk.choices[0].delta

                # Handle text content streaming
                if delta.content:
                    yield {
                        'type': 'token',
                        'content': delta.content
                    }

                # Handle tool calls streaming
                if delta.tool_calls:
                    for tool_call_delta in delta.tool_calls:
                        index = tool_call_delta.index

                        # Initialize tool call entry if needed
                        if index not in tool_calls_map:
                            tool_calls_map[index] = {
                                'id': tool_call_delta.id or '',
                                'name': '',
                                'arguments': ''
                            }

                        # Accumulate tool call data
                        if tool_call_delta.id:
                            tool_calls_map[index]['id'] = tool_call_delta.id

                        if tool_call_delta.function:
                            if tool_call_delta.function.name:
                                tool_calls_map[index]['name'] = tool_call_delta.function.name

                            if tool_call_delta.function.arguments:
                                tool_calls_map[index]['arguments'] += tool_call_delta.function.arguments

            # Execute tool calls and update graph
            for tool_call_data in tool_calls_map.values():
                if tool_call_data['name'] == 'create_task_node':
                    try:
                        # Parse the arguments
                        args = json.loads(tool_call_data['arguments'])

                        # Import the tool function
                        from .tools import create_task_node

                        # Execute the tool
                        node = create_task_node(
                            task_name=args['task_name'],
                            task_description=args['task_description'],
                            parent_id=args.get('parent_id')
                        )

                        # Update graph data
                        if len(graph_data["nodes"]) > 0 and node.get("parent_id"):
                            graph_data["links"].append({
                                "source": node["parent_id"],
                                "target": node["id"]
                            })

                        graph_data["nodes"].append({
                            "id": node["id"],
                            "name": node["name"],
                            "description": node["description"]
                        })

                        logger.info(f"Created task node: {node['name']}")

                    except Exception as e:
                        logger.error(f"Error executing tool call: {e}")
                        yield {
                            'type': 'token',
                            'content': f"\n\n[Error creating task: {str(e)}]"
                        }

            # Send final graph update
            yield {
                'type': 'graph_update',
                'graph_data': graph_data
            }

        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield {
                'type': 'token',
                'content': f"Sorry, I encountered an error: {str(e)}"
            }
